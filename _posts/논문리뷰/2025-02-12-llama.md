---
title:  "[논문리뷰] LLaMA: Open and Efficient Foundation Language Models"
last_modified_at: 2025-02-17
categories:
    - 논문리뷰
tag: 
    - AI
    - EE
    - Meta AI
    - LLM
    - NLP
excerpt: ""
use_math: true
classes: wide
---

>arXiv 2023. [[Paper](https://arxiv.org/abs/2302.13971)] [[Page](https://www.llama.com/)] [[Github](https://github.com/meta-llama/llama)]
>
>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
>
>Meta-llama | {htouvron, thibautlav,gizacard,egrave,glample}@meta.com
>
>27 Feb 2023


이 논문에서는 Meta AI(구 facebook research)에서 만든 LLaMA에 대해 소개한다. LLaMA는 7B to 65B의 파라미터를 가진 foundation language model이다. 이 모델은 수 조개의 토큰을 사용해 훈련했고, 공개적으로 이용 가능한 데이터셋만으로 최고 수준의 모델을 만들 수 있음을 보여주었다. 
 LLaMA-13B는 10배 넘는 크기의 GPT-3(175B)보다 대부분의 밴치마크에서 좋은 성능을 보였으며, LLaMA-65B는 최고의 모델인Chinchilla-70B, PaLM-540B과 경쟁할만한 성능을 보여주었다. 
 LLaMA는 모델들을 개발자 커뮤니티에 공개함으로써, 다양한 연구자들이 모델을 개선하고 최적화하는 데 필요한 자원과 기회를 제공하게 된 점에서도 의미가 크다.

# Instruction

 대규모 말뭉치로 학습된 LLM은 텍스트 지시나 몇 가지 예시(few-shot)만으로 새로운 작업을 수행하는 능력을 보여주었고 이 특성은 모델의 크기를 충분히 키웠을 때 나타타났다. 이 결과는 더 많은 파라미터가 더 좋은 성능을 이끈다는 가정을 만들었고, 모델의 파라미터를 더 키우는 방향으로 연구가 이어졌다. 하지만 [최근 연구](https://arxiv.org/abs/2203.15556)에서 같은 자원으로 가장 우수한 성능을 달성한 모델은 가장 큰 모델이 아닌, 더 많은 데이터로 훈련된 작은 모델이라는 사실을 보여주었다. 

 최근 연구의 scaling law의 목적은 특정 '훈련 자원'(training compute budget)에서 어떻게 데이터셋과 모델사이즈를 조절할지 결정하는 것이다. 하지만 이 목표는 '추론 자원'(inference budget)을 무시했고 이는 큰 모델을 서비스할 때 중요한 요소로 작용한다. 성능의 목표 수준을 고려할 때 선호되는 모델은 빠른 '훈련'이 아닌 빠른 '추론'이다. 그리고 특정 성능에 도달하기 위해 큰 모델을 훈련하는것이 비용이 저렴할 수 있지만, 작은 모델을 오래 훈련하는 것이 추론 비용에서 더 저렴할 수 있다. 그 예로 최근 연구에서 10B모델을 200B 토큰으로 훈련할 것을 권장했지만, 이 연구에서는 7B 모델이 1T 토큰을 사용한 후에도 성능이 향상함을 보였다.

 LLaMA는 더 많은 토큰을 사용해 다양한 추론 자원에서 가능한 최상의 성능을 달성하는 언어모델이 목표이고, 7B에서 65B개의 파라미터범위에 걸쳐 훈련 되었다. LLaMA-13B는 10배 작은 크기에도 GPT-3를 대부분의 밴치마크에서 능가했다. 그리고 이 모델은 단일 GPU에서 실행할 수 있기 때문에 LLM접근과 연구의 민주화에 도움이 될거라 추정한다. 뿐만 아니라 고성능 모델(65B)도 Chinchilla나 PaLM-540B와 같은 최고의 대형언어 모델과도 경쟁할 수 있다.

 이 연구진들은 기존의 모델들과 달리 공개된 데이터만 사용해 작업했고, 오픈 소싱과 호환되는 모델을 만들었다. OPT, GPT-NeoX, BLOOM, GLM등의 예외 모델들도 있지만, 이들은 Chinchilla와 PaLM-540B와 경쟁할 정도가 되지 못한다.



# Approach

 훈련 접근 방식은 이전연구 [(1)](https://arxiv.org/abs/2005.14165) [(2)](https://arxiv.org/abs/2005.14165)에서 설명된 방법과 유사하며, [Chinchilla scaling laws](https://arxiv.org/abs/2203.15556)의 에 영감을 받았다. 대량의 텍스트 데이터를 사용해 대형 트랜스포머 모델을 훈련하고, 이를 위해 Standard Optimizer를 사용했다.

## 1.Pre-training Data

 다음은 pre-training에 사용한 데이터셋들의 구성이다.

###  ![LLaMA_Table1_Pre_training_data]({{site.url}}/assets/img/2025-02-12-llama/LLaMA_Table1_Pre_training_data.png)        

 LLaMA의 훈련 데이터셋은 다양한 도메인을 비롯한 여러 자료들의 혼합으로 이루어졌다. 대부분 기존 LLM을 훈련하는데 사용한 데이터 소스를 재사용하되, 공개적으로 이용 가능하며 오픈소싱과 호환되는 데이터만 사용한다는 제한을 두었다.

 토크나이저로 Byte-Pair Encoding(BPE) 알고리즘을 사용했고 SentencePiece 구현을 활용했다. 모든 숫자는 개별 숫자로 분리되며, 알 수 없는 UTF-8 문자는 바이트 단위로 분해되었다.

 전체 학습 데이터는 토크나이징 후 약 1.4T개의 토큰을 포함한다. 대부분의 학습 데이터에서 각 토큰은 훈련 중 한 번만 사용되지만, Wikipedia와 Books 도메인에서 약 두게의 에폭 동안 학습이 진행 되었다.

## 2. Architecture

아래는 트랜스포머 아키텍쳐와 LLaMA의 아키텍쳐를 도식화한 모습이다.

<img src="{{site.url}}/assets/img/2025-02-12-llama/LLama_architecture_devopedia.org_llama.png" alt="LLama_architecture_devopedia.org_llama-llm" style="zoom:80%;" />

### Pre-nomalization[GPT3]

 RMSNorm을 활용해 각 트랜스포머 서브레이어의 출력을 정규화하는 대신 입력을 정규화하여 학습 안정성을 향상시켰다.

$$
a^{'}_i=\frac{a_i-\mu}{\sqrt{σ^2+\epsilon}}\cdot g_i
$$

기존 LayerNorm에서는 입력값 $a$ 의 평균과 분산을 계산해 정규화 한 후, 학습 가능한 스케일 파라미터 $g$ 를 적용했다. 여기서 $\mu$ 는 입력벡터 $a$의 평균, $\sigma$ 는 입력벡터의 분산, $g$는 학습 가능한 스케일의 파라미터, $\epsilon$은 수치적 안정성을 위한 작은 값이다.

$$
a^{'}_i=\frac{a_i}{RMS(a)}\cdot g_i
\space\cdot\space\cdot\space\cdot(RMS(a)=\sqrt{\frac{1}{n}\sum_{i=1}^n{a^2_i}}\space)
$$

 RSMNorm에서는 입력갑 $a$ 의 RMS를 계산해 정규화했다. 

 LayerNorm을 RMSNorm으로 바꾸면서 mean normalization을 제거하여 더 가볍게 만들어 계산량을 줄였고, 경량화모델이나 추론 최적화에 유리하게 만들었다.

### SwiGLU Activation Function [PaLM]

 Relu 비선형 함수를 성능 향상을 위해 SwiGLU활성화 함수로 교체하였고, PaLM에서 사용한 4d대신 $\frac{2}{3}\$차원을 사용했다.

### Rotary Embeddings[GPTNeo]

 이 모델은 절대적 위치 임베딩(absolute positional embeddings)을 제거하고 회전 위치 임베딩(RoPE)을 각 레이어에 추가했다.

## 3. Optimizer

 다음은 사용한 모델 사이즈와 아키텍쳐별 optimizer의 hyper-parameter이다.

 ![LLaMA_Table2_hyperparameters]({{site.url}}/assets/img/2025-02-12-llama/LLaMA_Table2_hyperparameters.png)



## 4. Efficient Implementation	

 모델의 훈련 속도향상을 위해 여러 가지 최적화를 적용하였다. 첫째, 메모리 사용량과 실행 시간을 줄이기 위해 인과 다중 헤드 어텐션(the casual multi-head attention)을 사용했다.이는 어텐션 가중치를 저장하지 않고, 언어 모델링 작업의 인과적인 특성으로 인해 마스킹된 키/쿼리 점수도 계산하지 않는다. 또한, 모델과 시퀀스 병렬성을 사용해 메모리 사용량을 줄이고, GPU간의 네트워크 통신을 퇴대한 겹쳐 계산하도록 최적화했다.

 65B 파라미터 모델을 훈련 할 때, 2048개의 A100 GPU(80GB RAM)을 사용해 초당 380개 토큰을 처리했고, 이는 1.4T 토큰이 포한된 데이터셋을 훈련하는데 약 21일이 걸린다는 의미이다.

# Main result

 [이전 연구](https://arxiv.org/abs/2005.14165)에 따라, zero-shot, few-shot 작업을 고려하고 총 20개의 벤치마크에서의 결과를 보여주었다.

### 1. Common Sense Reasoning

다음은 zero-shot으로 모델을 다양한 밴치마크한 결과이다.

 zero-shot으로 모델들을 비교했다. LLaMA-65B는 BoolQ를 제외한 밴치마크에서 Chinchilla보다 좋은 성능을 보였고, BoolQ와 WinoGrande를 제외한 밴치마크에서 PalM(540B)보다 좋은 성능을 보였다. 게다가 GPT-3보다 10배 작은 크기임에도 불구하고 모든 밴치마크에서 능가했다.

### 2. Closed-book Question Answering

다음은 두개의 closed-book question answering인 Natural Questions와 TriviaQA를 밴치마크한 결과이다.

![LLaMA_Table4n5]({{site.url}}/assets/img/2025-02-12-llama/LLaMA_Table4n5.png)

### 3. Reading Comprehension

다음은 중국 중,고등학교 시험문제에 대한 독해력(Reading Comprehension)의 밴치마크 결과이다.

![LLaMA_Table6]({{site.url}}/assets/img/2025-02-12-llama/LLaMA_Table6.png)

### 4. Mathematical Reasoning

다음은 수학적 추론 MATH와 GSM8k밴치마크 결과이다. 

![LLaMA_Table7]({{site.url}}/assets/img/2025-02-12-llama/LLaMA_Table7.png)

### 5. Code Generation

다음은 코드 작성에 대한 밴치마크인 HumanEval과 MBPP의 결과이다.

![LLaMA_Table8]({{site.url}}/assets/img/2025-02-12-llama/LLaMA_Table8.png)

### 6. Massive Multitask Language Understanding

다음은 대규모 다중작업 언어 이해에 대한 밴치마크 Humanities, STEM, Social Sciences의 결과이다.

![LLaMA_Table9]({{site.url}}/assets/img/2025-02-12-llama/LLaMA_Table9.png)

### 7. Evolution of Performance During Training

 다음은 모델의 크기별 훈련 토큰에 따른 훈련 손실(Training loss)의 상관 관계를 보여주는 그래프이다. 

![LLaMA_Figure1]({{site.url}}/assets/img/2025-02-12-llama/LLaMA_Figure1.png)

 다음은 각 밴치마크별로 훈련토큰과 정확도의 산관관계를 보여주는 그래프이다. 대부분 성능은 꾸준히 향상됨을 보이고 있다. 하지만 SIQA에서는 큰 변동성이 있어 신뢰하기 어렵고, WinoGrande에서는 성능이 훈련의 불확실성(perplexity)와 잘 맞지 않고 앞선 자료와 맞지 않는다. 특히 LLaMA-33B와 LLaMA-65B는 훈련중에 비슷한 성능을 보이고 있다.

![LLaMA_Figure2]({{site.url}}/assets/img/2025-02-12-llama/LLaMA_Figure2.png)

## 결론

 이 논문에서는 공개적으로 배포된 언어모델을 소개하며, 최고성능의 모델들과 경쟁할 수 있는 성능을 갖춘 모델을 제시했다.

 LLaMA-13B가 GPT-3다 10배 더 작음에도 뛰어난 성능을 보인것과 LLaMA-65B가 Chinchilla-70B및 PaLM-540와 경쟁할 만한 성능을 보여 최고 성능의 모델들과 경쟁할 수 있음을 보였다..

 공개적으로 이용 가능한 데이터만으로도 최고 성능을 달성할 수 있음을 보이고, 이 모델을 연구 커뮤니티에 공유함으로써 대형 언어모델 개발을 가속화 할것으로 전망 했다.

 훈련 데이터셋의 규모를 확장함에 따라 지속적인 성능 형상을 확인해, 더 큰 사전훈련 데이터(corpora)를 활용하려한다.
